{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddedbee-14d5-43b7-ba02-ad364022e595",
   "metadata": {},
   "source": [
    "# Understanding the Social Networks of Emma B. Andrews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a92b3-bba4-4bad-b9ad-2b50e7406c14",
   "metadata": {},
   "source": [
    "In this notebook, we will use Python to parse the Emma B. Andrews diaries TEI files. Our interest is to visualise the social networks in the life of Emma B. Andrews life on the nile. To understand these networks, we will use several text mining features to extract TEI elements (`<persName>`) as well as analyse the grammatical structure of the jounral entry to explore the social graph of Emma B. Andrews.\n",
    "\n",
    "To accomplish our work, we will use several modules. The modules are as follows:\n",
    "\n",
    "* [csv](https://docs.python.org/3/library/csv.html)\n",
    "* [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [lxml](https://pypi.org/project/lxml/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [nltk](https://www.nltk.org/)\n",
    "\n",
    "The Beautiful Soup, lxml, NLTK, and Matplotlib modules need to be installed. If you are running Jupyter Notebook with the Python Virtual Environment, then these modules were installed when you created the Virtual Environment. The `csv` module comes preinstalled in the Python virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee965f-6809-4d95-9c82-eed77b930611",
   "metadata": {},
   "source": [
    "## Import Modules (Dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2c6b9-d2e4-4309-ad76-170bf5701e57",
   "metadata": {},
   "source": [
    "To import a module, we will use the Python import function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426af1b7-0c36-4f32-b1d8-4318c4d9ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # Python's Comma Separate Values Parser\n",
    "from bs4 import BeautifulSoup # Beautiful Soup is for parsing HTML and XML files\n",
    "import lxml # lxml is a secondary parser for beautiful soup\n",
    "import nltk # Natural Langauge Toolkit\n",
    "import re # Python's Regular Expression Module\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a57a6-314b-4938-872a-de30757abeb5",
   "metadata": {},
   "source": [
    "## Read Volume into Python to Parse with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d60424-46c6-4599-9aaf-aa4cd47bcba6",
   "metadata": {},
   "source": [
    "The tagged TEI files of the Journals are located in the `/diary-volumes` directory. We need to tell Python the source of the file. We will want to use the Python OS module to make this work for either Windows or Mac. For now, it is hard coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfcfb0a-2385-4d75-9d10-28f898d0eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the journal volume we want exists in the same directory as our Jupyter Notebook, we can use the document name with extension.\n",
    "journal = '../diary-volumes/volume17.xml'\n",
    "\n",
    "# Now we want to create a Beautiful Soup object with our file. We will unpack what this means in more detail below.\n",
    "with open(journal) as xml:\n",
    "    soup = BeautifulSoup(xml, 'lxml-xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18816da6-a628-482b-b82a-f39ad44e76a3",
   "metadata": {},
   "source": [
    "## Extract Diary Entry for Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe090f-dddf-45d4-b1be-3250df2a57ca",
   "metadata": {},
   "source": [
    "The Diaries are encoded according to the `TEI` standards. Thus, the `<text>…</text>` element encloses the contents of the dairy. We want to parse every day of the dairy and then further manipulate the data for Graph Analysis. Each child within the `<text>` root is an entry according to the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9c17b-5cc8-454a-afb0-e7f57e1309ae",
   "metadata": {},
   "source": [
    "### Format Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6c386e-9e95-4e59-968b-c1a05e4c80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the daily entries, we need to traverse the text root and gather together all <div> elements with a type of entry. To do this, we will use\n",
    "# the beautiful soup library. While we are working through each entry, we also pass each entry through the sentiment analysis\n",
    "# to score the entirety of the entry. We will improve on sentiment in connection to people at another step.\n",
    "\n",
    "# Set the Lexicon for Afinn Lexicon\n",
    "afinn = Afinn(language='en')\n",
    "\n",
    "with open('networks.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Date\", \"Name\", \"Relation\", \"Entry\", \"Entry_Sentiment\"])\n",
    "    for i in soup.find_all(\"div\", {\"type\": \"entry\"}):\n",
    "        \n",
    "        #Extract the Date for the Graph Model\n",
    "        match = re.search('EBA-([0-9-–]+)', i.attrs['xml:id'])\n",
    "        date = match.group(1) if match else None\n",
    "\n",
    "        #Clean the Entry and Prepare for Post-Processing\n",
    "        remove_newlines = re.sub(\"\\n+\", \" \", i.text.strip())\n",
    "        plain_text = re.sub(\" +\", \" \", remove_newlines)\n",
    "        \n",
    "        #Extract all the PersName and Score Entry in which PersName appears\n",
    "        people = i.find_all('persName')\n",
    "        if not people:\n",
    "            writer.writerow([date, \"None\", \"None\", \"None\", \"None\"])\n",
    "        else:\n",
    "            for p in people:\n",
    "                if ' ' in p['ref']:\n",
    "                    a = p['ref'].split(' ')\n",
    "                    for b in a:                    \n",
    "                        afinn_scr = afinn.score(plain_text)\n",
    "                        writer.writerow([date, b, \"\", plain_text, afinn_scr])\n",
    "                else:\n",
    "                    afinn_scr = afinn.score(plain_text)\n",
    "                    writer.writerow([date, p['ref'], \"\", plain_text, afinn_scr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b3fc0-d241-44ca-86f4-ea10da718094",
   "metadata": {},
   "source": [
    "### Extract the persName Entities from Each Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29898a-ccde-44fc-a2f8-904b715cbd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
