{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddedbee-14d5-43b7-ba02-ad364022e595",
   "metadata": {},
   "source": [
    "# Understanding the Social Networks of Emma B. Andrews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a92b3-bba4-4bad-b9ad-2b50e7406c14",
   "metadata": {},
   "source": [
    "In this notebook, we want to use Python to parse the Emma B. Andrews diaries TEI files. Our interest is to visualise the social networks in the life of Emma B. Andrews life on the nile. To parse the TEI documents, we will use several modules. These are as follows:\n",
    "\n",
    "* [csv](https://docs.python.org/3/library/csv.html)\n",
    "* [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [lxml](https://pypi.org/project/lxml/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [nltk](https://www.nltk.org/)\n",
    "\n",
    "The Beautiful Soup, lxml, NLTK, and Matplotlib modules need to be installed. If you are running Jupyter Notebook with the Python Virtual Environment, then these modules were installed when you created the Virtual Environment. The `csv` module comes preinstalled in the Python virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee965f-6809-4d95-9c82-eed77b930611",
   "metadata": {},
   "source": [
    "## Import Modules (Dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2c6b9-d2e4-4309-ad76-170bf5701e57",
   "metadata": {},
   "source": [
    "To import a module, we will use the Python import function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "426af1b7-0c36-4f32-b1d8-4318c4d9ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # Python's Comma Separate Values Parser\n",
    "from bs4 import BeautifulSoup # Beautiful Soup is for parsing HTML and XML files\n",
    "import lxml # lxml is a secondary parser for beautiful soup\n",
    "import nltk # Natural Langauge Toolkit\n",
    "import re # Python's Regular Expression Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a57a6-314b-4938-872a-de30757abeb5",
   "metadata": {},
   "source": [
    "## Read Volume into Python to Parse with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d60424-46c6-4599-9aaf-aa4cd47bcba6",
   "metadata": {},
   "source": [
    "The tagged TEI files of the Journals are located in the `/diary-volumes` directory. We need to tell Python the source of the file. We will want to use the Python OS module to make this work for either Windows or Mac. For now, it is hard coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfcfb0a-2385-4d75-9d10-28f898d0eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the journal volume we want exists in the same directory as our Jupyter Notebook, we can use the document name with extension.\n",
    "journal = '../diary-volumes/volume17.xml'\n",
    "\n",
    "# Now we want to create a Beautiful Soup object with our file. We will unpack what this means in more detail below.\n",
    "with open(journal) as xml:\n",
    "    soup = BeautifulSoup(xml, 'lxml-xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18816da6-a628-482b-b82a-f39ad44e76a3",
   "metadata": {},
   "source": [
    "## Parse Diary for Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe090f-dddf-45d4-b1be-3250df2a57ca",
   "metadata": {},
   "source": [
    "The Diaries are encoded according to the `TEI` standards. Thus, the `<text>…</text>` element encloses the contents of the dairy. We want to parse every day of the dairy and then further manipulate the data for Graph Analysis. Each child within the `<text>` root is an entry according to the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9c17b-5cc8-454a-afb0-e7f57e1309ae",
   "metadata": {},
   "source": [
    "### Extract Daily Entries from Volume 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6c386e-9e95-4e59-968b-c1a05e4c80ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volume contains 43 entries'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To extract the daily entries, we need to traverse the text root and gather together all <div> elements with a type of entry\n",
    "entries = soup.find_all(\"div\", {\"type\": \"entry\"}) #find all div elements whose type attribute is entry (this is a journal entry)\n",
    "num_entries = len(entries) #Count the entries\n",
    "f'Volume contains {num_entries} entries' #Discover total entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92ac1a-a944-4bf1-b447-4c05376bbdc2",
   "metadata": {},
   "source": [
    "### Extract the Dates of the Entries and Create a Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef4c677-bdd7-4197-ad36-1dc0b86af9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volume contains 43 entries'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over the entries and get the Date for each\n",
    "dates = soup.find_all(attrs={\"xml:id\": re.compile(\"EBA-[0-9-–]+\")}) #Find all dates in the entries with a regular expression search\n",
    "total_dates = len(dates) #Count the dates -- this should equal the amount of entries. If not, there is either an encoding issue or Andrews did not date the entry\n",
    "f'Volume contains {total_dates} entries' #Discover total entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b3fc0-d241-44ca-86f4-ea10da718094",
   "metadata": {},
   "source": [
    "### Extract the persName Entities from Each Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44d96f2-9f12-415f-925e-f1df6912275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a List of All the People\n",
    "network = []\n",
    "for entry in entries:\n",
    "    peoples = entry.find_all('persName')\n",
    "    for person in peoples:\n",
    "        network.append(person['ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094cd797-ae77-4bf6-935f-693d2427be78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Troyon_Constant',\n",
       " '#Corot_Jean_Baptiste_Camille',\n",
       " '#Rathbone_Mr',\n",
       " '#Rathbone_Mr #Rathbone_Mrs',\n",
       " '#Rathbone_Elena',\n",
       " '#Parsons_John #Parsons_Florence_Van_Corltandt',\n",
       " '#Draper_Mr',\n",
       " '#Gorst_Group',\n",
       " '#Trefusis_Walter',\n",
       " '#Carter_Bonham_Mr',\n",
       " '#Rathbone_Elena',\n",
       " '#Lovatt_Mr #Lovatt_Master',\n",
       " '#Lovatt_Mr #Lovatt_Master',\n",
       " '#Gay_Walter',\n",
       " '#Gay_Walter_Mr #Gay_Mrs',\n",
       " '#Gay_Mrs',\n",
       " '#Rathbone_Elena',\n",
       " '#Buckley_Mr #Buckley_Mrs',\n",
       " '#Carter_Howard',\n",
       " '#Weigall_Arthur',\n",
       " '#Nicol_Erskine',\n",
       " '#Davis_Theodore_M',\n",
       " '#Weigall_Arthur',\n",
       " '#Butler_Mrs',\n",
       " '#Nicol_Erskine',\n",
       " '#Butler_Mrs',\n",
       " '#Weigall_Arthur',\n",
       " '#Davis_Theodore_M',\n",
       " '#Burton_Harry',\n",
       " '#Maspero_Gaston',\n",
       " '#Davis_Theodore_M',\n",
       " '#Davis_Theodore_M',\n",
       " '#Jones_Harold',\n",
       " '#Jones_Cyril',\n",
       " '#Rathbone_Elena',\n",
       " '#Davis_Theodore_M',\n",
       " '#Carter_Howard',\n",
       " '#Nicol_Erskine',\n",
       " '#Davis_Theodore_M',\n",
       " '#Jones_Harold',\n",
       " '#Jones_Cyril',\n",
       " '#Jones_Harold',\n",
       " '#Mumm_von_Schwarzenstein_Alfons',\n",
       " '#Fahnestock_Gibson #Fahnestock_Mrs',\n",
       " '#Kelly_Miss',\n",
       " '#Whitaker_Mr',\n",
       " '#Whymper_Charles',\n",
       " '#Kelly_Mr #Kelly_Miss',\n",
       " '#Burton_Harry',\n",
       " '#Davis_Theodore_M',\n",
       " '#Crane_Lancelot',\n",
       " '#Horemheb',\n",
       " '#Trefusis_Walter',\n",
       " '#Davis_Theodore_M',\n",
       " '#Whitaker_Mr',\n",
       " '#Newberry_Percy #Newberry_Mrs',\n",
       " '#Maspero_Gaston #Maspero_Louise',\n",
       " '#Davis_Theodore_M',\n",
       " '#Maspero_Mme',\n",
       " '#McCormick_Mrs',\n",
       " '#Scott_Miss',\n",
       " '#Buckley_Mr #Buckley_Mrs',\n",
       " '#Carter_Howard',\n",
       " '#Burton_Harry',\n",
       " '#Davis_Theodore_M',\n",
       " '#Gorst_Lady',\n",
       " '#Gorst_Miss',\n",
       " '#Hunter_Mrs',\n",
       " '#Warner_Mrs #Warner_Miss',\n",
       " '#Webb_Miss',\n",
       " '#Guadalmina_Marquesa #Guadalmina_son_of',\n",
       " '#Davis_Theodore_M',\n",
       " '#Mohassib_Mohammed',\n",
       " '#Cassatt_Mary_S',\n",
       " '#Kelekian_Mr #Kelekian_Mrs',\n",
       " '#Nicol_Erskine',\n",
       " '#Peabody_Endicott',\n",
       " '#Fairfield_Osborn_Henry',\n",
       " '#Newberry_Percy #Newberry_Mrs',\n",
       " '#Fahnestock_Gibson #Fahnestock_Mrs',\n",
       " '#Davis_Theodore_M',\n",
       " '#Alexander_Charles #Alexander_Mrs',\n",
       " '#Alexander_girls',\n",
       " '#Hobhouse_Henry #Hobhouse_Mrs',\n",
       " '#Farrer_Gaspard',\n",
       " '#Williams_Mr #Williams_Mrs',\n",
       " '#Langley_Mr #Langley_Mrs',\n",
       " '#Foster_Giraud #Foster_Mrs',\n",
       " '#Ives_Miss',\n",
       " '#Alexander_Mrs',\n",
       " 'Roosevelt_Theodore_Jr',\n",
       " '#Collander_Livingston_John #Collander_Livingston_Mrs',\n",
       " '#Davis_Theodore_M',\n",
       " '#Whymper_Charles',\n",
       " '#Foster_Giraud #Foster_Mrs',\n",
       " '#Graham_Mrs',\n",
       " '#Collander_Livingston_John #Collander_Livingston_Mrs',\n",
       " '#Hamilton_Fish_Webster_Mrs',\n",
       " '#Auchincloss_Mr #Auchincloss_Mrs',\n",
       " '#Jennings_Miss',\n",
       " '#Naville_Edouard #Naville_Marguerite',\n",
       " '#Rodier_M',\n",
       " '#Cherry_Mrs #Cherry_Miss',\n",
       " '#Cust_Mr',\n",
       " '#Maspero_Gaston #Maspero_Louise',\n",
       " '#Davis_Theodore_M',\n",
       " '#Carter_Howard',\n",
       " '#Carnarvon_Lord',\n",
       " '#Hobhouse_Henry #Hobhouse_Mrs',\n",
       " '#Farrer_Gaspard',\n",
       " '#Davis_Theodore_M',\n",
       " 'Morgan_John_Pierpoint',\n",
       " '#Davis_Theodore_M',\n",
       " '#Layard_Lady',\n",
       " '#Nicol_Erskine',\n",
       " 'Morgan_John_Pierpoint',\n",
       " '#Davis_Theodore_M',\n",
       " '#Auchincloss_Mrs',\n",
       " '#Jennings_Miss',\n",
       " '#Contardone_Contessa',\n",
       " '#Rathbone_Elena',\n",
       " '#Newberry_Percy #Newberry_Mrs',\n",
       " '#Johnson_Mr',\n",
       " '#Whymper_Charles',\n",
       " '#Nicol_Erskine',\n",
       " '#Burton_Harry',\n",
       " '#Rathbone_Elena',\n",
       " '#Rathbone_Elena',\n",
       " '#Naville_Edouard #Naville_Marguerite',\n",
       " '#Naville_Mme',\n",
       " '#Naville_Edouard #Naville_Marguerite',\n",
       " '#Whitmore_Mr',\n",
       " '#Dixon_Mr',\n",
       " '#Naville_Edouard #Naville_Marguerite',\n",
       " '#Rathbone_Elena',\n",
       " '#Akhenaten',\n",
       " '#Rathbone_Elena',\n",
       " '#Burton_Harry',\n",
       " '#Jones_Harold',\n",
       " '#Draper_Mr',\n",
       " '#Rathbone_Elena',\n",
       " '#Newberry_Percy_E',\n",
       " '#Deimer_Michael_Z',\n",
       " '#Kelekian_Dikran',\n",
       " '#Kassera',\n",
       " '#Davis_Theodore_M',\n",
       " '#Burton_Harry',\n",
       " '#Nachman',\n",
       " '#Rathbone_Elena',\n",
       " '#Burton_Harry',\n",
       " '#Davis_Theodore_M',\n",
       " '#Rathbone_Elena',\n",
       " '#Burton_Harry',\n",
       " '#Winlock_Herbert',\n",
       " '#Davis_Theodore_M',\n",
       " '#Kyticas_N_D',\n",
       " '#Davis_Theodore_M',\n",
       " '#Newberry_Percy #Newberry_Mrs',\n",
       " '#Johnson_Mr',\n",
       " '#Whittaker_Mr #Whittaker_Mrs',\n",
       " '#Layard_Lady',\n",
       " '#Johnson_Mr',\n",
       " '#Trefusis_Walter',\n",
       " '#Davis_Theodore_M',\n",
       " '#Pasha_Artin',\n",
       " '#Sayce_Archibald',\n",
       " '#Kyticas_N_D',\n",
       " '#Daressy_M',\n",
       " '#Tiye',\n",
       " '#Davis_Theodore_M',\n",
       " '#Duvar_Mrs',\n",
       " '#Rathbone_Elena',\n",
       " '#Bonham_Carter_Mr',\n",
       " '#Burton_Harry',\n",
       " '#Whymper_Charles',\n",
       " '#Davis_Theodore_M',\n",
       " '#Pasha_Artin',\n",
       " '#Sayce_Archibald',\n",
       " '#Bonham_Carter_Mr',\n",
       " '#Trefusis_Walter',\n",
       " '#Graham_Margery',\n",
       " '#Graham_John',\n",
       " '#Northampton_Lord',\n",
       " '#Duvar_Mrs',\n",
       " '#Coater_Miss']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is temporary. The next step is associate the date with each person. This will create a timeline of when Andrews encountered and wrote about the named person.\n",
    "# Once we zip together the date with the person, we will process the text of the entry with NLTK to discover the verbal association between Andrews and named person.\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a13aa-8afe-4f9a-984e-9c0e6c14546f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
