{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddedbee-14d5-43b7-ba02-ad364022e595",
   "metadata": {},
   "source": [
    "# Understanding the Social Networks of Emma B. Andrews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a92b3-bba4-4bad-b9ad-2b50e7406c14",
   "metadata": {},
   "source": [
    "In this notebook, we will use Python to parse the Emma B. Andrews diaries TEI files. Our interest is to visualise the social networks in the life of Emma B. Andrews life on the nile. To understand these networks, we will use several text mining features to extract TEI elements (`<persName>`) as well as analyse the grammatical structure of the jounral entry to explore the social graph of Emma B. Andrews.\n",
    "\n",
    "To accomplish our work, we will use several modules. The modules are as follows:\n",
    "\n",
    "* [csv](https://docs.python.org/3/library/csv.html)\n",
    "* [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [lxml](https://pypi.org/project/lxml/)\n",
    "* [Afinn](https://pypi.org/project/afinn/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [nltk](https://www.nltk.org/)\n",
    "\n",
    "The Beautiful Soup, lxml, Afinn, NLTK, and Matplotlib modules need to be installed. If you are running Jupyter Notebook with the Python Virtual Environment, then these modules were installed when you created the Virtual Environment. The `csv` module comes preinstalled in the Python virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee965f-6809-4d95-9c82-eed77b930611",
   "metadata": {},
   "source": [
    "## Import Modules (Dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2c6b9-d2e4-4309-ad76-170bf5701e57",
   "metadata": {},
   "source": [
    "To import a module, we will use the Python import function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426af1b7-0c36-4f32-b1d8-4318c4d9ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # Python's Comma Separate Values Parser\n",
    "from bs4 import BeautifulSoup # Beautiful Soup is for parsing HTML and XML files\n",
    "import lxml # lxml is a secondary parser for beautiful soup\n",
    "import nltk # Natural Langauge Toolkit\n",
    "import re # Python's Regular Expression Module\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a57a6-314b-4938-872a-de30757abeb5",
   "metadata": {},
   "source": [
    "## Read Volume into Python to Parse with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d60424-46c6-4599-9aaf-aa4cd47bcba6",
   "metadata": {},
   "source": [
    "The tagged TEI files of the Journals are located in the `/diary-volumes` directory. We need to tell Python the source of the file. We will want to use the Python OS module to make this work for either Windows or Mac. For now, it is hard coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfcfb0a-2385-4d75-9d10-28f898d0eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the journal volume we want exists in the same directory as our Jupyter Notebook, we can use the document name with extension.\n",
    "journal = '../diary-volumes/volume17.xml'\n",
    "\n",
    "# Now we want to create a Beautiful Soup object with our file. We will unpack what this means in more detail below.\n",
    "with open(journal) as xml:\n",
    "    soup = BeautifulSoup(xml, 'lxml-xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18816da6-a628-482b-b82a-f39ad44e76a3",
   "metadata": {},
   "source": [
    "## Analyse Diary and Create Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe090f-dddf-45d4-b1be-3250df2a57ca",
   "metadata": {},
   "source": [
    "The Diaries are encoded according to the `TEI` standards. Thus, the `<text>…</text>` element encloses the contents of the dairy. We want to parse every day of the dairy and then further manipulate the data for Graph Analysis. As of now, a core interest is to understand the social network of Emma B. Andrews. Thus, we will process each volume, in order to create a data model. We can further use the data model in Gephi to visually represent the social network along our nodes of data. At this stage our nodes are:\n",
    "\n",
    "* The date of writing. This is not necessarily the date Emma encountered the person, but the day she wrote about the person.\n",
    "* The name of the person. Here we rely on the TEI tagging element of `persName` to aggregate each person as a node.\n",
    "* Entry text. We strip the text of the TEI at this point and keep the person anchored to the writing of Emma B. Andrews. We will use the Entry text for further natural language processing to populate the relation field. (See discussion below)\n",
    "* We analyse the sentiment of each entry for its overall score. We store the sentiment score in Entry_Sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9c17b-5cc8-454a-afb0-e7f57e1309ae",
   "metadata": {},
   "source": [
    "### Format Entries\n",
    "\n",
    "Each child within the `<text>` root is an entry according to the day. Thus, we need to iterate over the `div` elements within root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6c386e-9e95-4e59-968b-c1a05e4c80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the daily entries, we need to traverse the text root and gather together all <div> elements with a type of entry. To do this, we will use\n",
    "# the beautiful soup library. While we are working through each entry, we also pass each entry through the sentiment analysis\n",
    "# to score the entirety of the entry. We will improve on sentiment in connection to people at another step.\n",
    "\n",
    "# Set the Lexicon for Afinn Lexicon\n",
    "afinn = Afinn(language='en')\n",
    "\n",
    "with open('networks.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Date\", \"Name\", \"Relation\", \"Entry\", \"XML\", \"Entry_Sentiment\"])\n",
    "    for i in soup.find_all(\"div\", {\"type\": \"entry\"}):\n",
    "        \n",
    "        # Strip White Space from XML\n",
    "        txt_string = str(i)\n",
    "        xml = \"\".join(line.strip() for line in txt_string.split(\"\\n+\"))\n",
    "    \n",
    "        #Extract the Date for the Graph Model\n",
    "        match = re.search('EBA-([0-9-–]+)', i.attrs['xml:id'])\n",
    "        date = match.group(1) if match else None\n",
    "\n",
    "        #Clean the Entry and Prepare for Post-Processing\n",
    "        remove_newlines = re.sub(\"\\n+\", \" \", i.text.strip())\n",
    "        plain_text = re.sub(\" +\", \" \", remove_newlines)\n",
    "        \n",
    "        #Extract all the PersName and Score Entry in which PersName appears\n",
    "        people = i.find_all('persName')\n",
    "        if not people:\n",
    "            writer.writerow([date, \"None\", \"None\", \"None\", \"None\", \"None\"])\n",
    "        else:\n",
    "            for p in people:\n",
    "                if ' ' in p['ref']:\n",
    "                    a = p['ref'].split(' ')\n",
    "                    for b in a:                    \n",
    "                        afinn_scr = afinn.score(plain_text)\n",
    "                        writer.writerow([date, b, \"\", plain_text, xml, afinn_scr])\n",
    "                else:\n",
    "                    afinn_scr = afinn.score(plain_text)\n",
    "                    writer.writerow([date, p['ref'], \"\", plain_text, xml, afinn_scr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b3fc0-d241-44ca-86f4-ea10da718094",
   "metadata": {},
   "source": [
    "## Relational Analysis\n",
    "\n",
    "We are now in a position to analyse the entry so as to extract additional information about the relationship between Emma B. Andrews and her social networks. We are going to use natural language processing tools to analyse the language Emma B. Andrews used apropos each person of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f4710-6667-4083-b60e-852ce288c060",
   "metadata": {},
   "source": [
    "### Process the Entry Language to Discern Type of Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f122f49-fe0f-4fb0-99cf-53c033a0a7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
